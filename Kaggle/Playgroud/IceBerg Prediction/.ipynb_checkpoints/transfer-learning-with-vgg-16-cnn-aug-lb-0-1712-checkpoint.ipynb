{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "7922e149-72fa-48cd-a545-b70bddb40d28",
    "_uuid": "9341f2516086ca38bca96e06a9dbfc39f813a95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv\n",
      "test.json\n",
      "train.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"/Users/szkfzx/datasets/statoil-iceberg-classifier-challenge\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f06173b0-d354-4693-904e-a87f840ec372",
    "_uuid": "41648b358ed7c6c682ad50b85a347595733e2214"
   },
   "source": [
    "#### TL DR\n",
    "**Runs on GPU** There is some compatiblity issue with CPUs\n",
    "1. Hyperparameters in Deep learning are many, tuning them will take weeks or months. Generally researchers do this tuning and publish paper when they find a nice set of architecture which performs better than other.\n",
    "\n",
    "2. Since the model is pre-trained, it converges very fast and you but still you need GPU to use this. Due to some library issues, it doesn't work on CPU.\n",
    "\n",
    "2. For our purpose, we can use those architectures, which are made available by those researchers to us.\n",
    "\n",
    "3. Using those pretrained nets, layers of which already 'knows' how to extract features, we can don't have to tune the hyperparameters. Since they are already trained of some dataset(say imagenet), their pre-trained weights provide a good initialization of weights and because of this, our Convnet converges very fast which otherwise can take days on these deep architectures. That's the idea behind **Transfer Learning**.  Examples of which are VGG16, InceptionNet, goolenet, Resnet etc.\n",
    "\n",
    "4. In this kernel we will use pretrained VGG-16 network which performs very well on small size images.\n",
    "\n",
    "5.** VGG architecture has proved to worked well on small sized images(CIFAR-10)  ** I expected it to work well for this dataset as well.\n",
    "\n",
    "6. The code also includes the data augmentation steps, thus considerably improving the performance.\n",
    "\n",
    "- **GPU is needed**\n",
    "\n",
    "Here is the link of the research paper if you are interested.\n",
    "[https://arxiv.org/pdf/1409.1556.pdf](http://)\n",
    "\n",
    "Also here is the doc for keras library:\n",
    "[https://keras.io/applications/#vgg16](http://)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "42618ab1-ea7d-4a8b-98ae-584774306108",
    "_uuid": "c73fefe3108dfd1f26cfda367a8df7283ac4c586",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mandatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "9d5839b6-f7fb-426d-b05f-cf2dba9313a8",
    "_uuid": "d375d4f754ad7fb77db2142c7c075b4ad4168390",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"/Users/szkfzx/datasets/statoil-iceberg-classifier-challenge/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "test = pd.read_json(\"/Users/szkfzx/datasets/statoil-iceberg-classifier-challenge/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3cfe5ed2-f8f7-462b-92a9-b2a3a5b40978",
    "_uuid": "88e01459499ec6e3b007b800c9bae0623f3dce7f"
   },
   "source": [
    "\n",
    "\n",
    "Keras provide the implementation of pretrained VGG, it in it's library so we don't have to build the net by ourselves.\n",
    "Here we are removing the last layer of VGG and putting our sigmoid layer for binary predictions.\n",
    "\n",
    "The following code will NOT WORK, since on kaggle notebook, the weights of model cannot be downloaded, however, you can copy paste the code in your own notebook to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "X_test_angle=test['inc_angle']\n",
    "\n",
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_band_3=(X_band_1+X_band_2)/2\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras.\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\t\n",
    "\n",
    "#Data Aug for multi-input\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.2,\n",
    "                         rotation_range = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "067f3dd7-3dcf-4b71-857d-e00b4afbd06e",
    "_uuid": "af8be6ce23dba815bbde23fd7e196eb54ae7c4e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "def getVggAngleModel():\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1, )(input_2)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    merge_one = concatenate([x, angle_layer])\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "    \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def myAngleCV(X_train, X_angle, X_test):\n",
    "    K=3\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"/Users/szkfzx/datasets/vggnet/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n",
    "        galaxyModel= getVggAngleModel()\n",
    "        galaxyModel.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        galaxyModel.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = galaxyModel.evaluate([X_train_cv,X_angle_cv], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        #Getting Test Score\n",
    "        score = galaxyModel.evaluate([X_holdout,X_angle_hold], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=galaxyModel.predict([X_holdout,X_angle_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "        temp_test=galaxyModel.predict([X_test, X_test_angle])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=galaxyModel.predict([X_train, X_angle])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n",
    "    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n",
    "    return y_test_pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ea82458f-f41c-4abb-87aa-0dfc7a447969",
    "_uuid": "d462c689ee61d4c1cdcee42c7ded6c7c31c9cddc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================FOLD= 0\n",
      "WARNING:tensorflow:From /Users/szkfzx/anaconda2/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/szkfzx/anaconda2/envs/python36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/szkfzx/anaconda2/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szkfzx/anaconda2/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/szkfzx/anaconda2/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 119s 5s/step - loss: 0.7322 - acc: 0.5785 - val_loss: 0.4856 - val_acc: 0.7346\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 111s 5s/step - loss: 0.8156 - acc: 0.5321 - val_loss: 0.6923 - val_acc: 0.5308\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 112s 5s/step - loss: 0.7317 - acc: 0.5004 - val_loss: 0.6919 - val_acc: 0.5308\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 113s 5s/step - loss: 0.7179 - acc: 0.5053 - val_loss: 0.6942 - val_acc: 0.4692\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: 0.7055 - acc: 0.5107 - val_loss: 0.6918 - val_acc: 0.5308\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: 0.7051 - acc: 0.5071 - val_loss: 0.7019 - val_acc: 0.4692\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: 0.7139 - acc: 0.4907 - val_loss: 0.7002 - val_acc: 0.4692\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 108s 4s/step - loss: 0.7103 - acc: 0.4936 - val_loss: 0.6951 - val_acc: 0.4692\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: 0.6972 - acc: 0.5231 - val_loss: 0.6952 - val_acc: 0.4692\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: 0.6983 - acc: 0.5219 - val_loss: 0.6918 - val_acc: 0.5308\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 110s 5s/step - loss: 0.7020 - acc: 0.5054 - val_loss: 0.6970 - val_acc: 0.4692\n",
      "Train loss: 0.49384791302948605\n",
      "Train accuracy: 0.7502338634237605\n",
      "Test loss: 0.4856415083475202\n",
      "Test accuracy: 0.7345794392523365\n",
      "\n",
      "===================FOLD= 1\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 110s 5s/step - loss: 0.7835 - acc: 0.5879 - val_loss: 0.5119 - val_acc: 0.7402\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: 0.5521 - acc: 0.6912 - val_loss: 0.3867 - val_acc: 0.8056\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 107s 4s/step - loss: 0.4985 - acc: 0.7416 - val_loss: 0.3702 - val_acc: 0.8037\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 111s 5s/step - loss: 0.5528 - acc: 0.7225 - val_loss: 0.5217 - val_acc: 0.6841\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 111s 5s/step - loss: nan - acc: 0.2643 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 110s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 107s 4s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 107s 4s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Train loss: 0.366739789747099\n",
      "Train accuracy: 0.7998129095397785\n",
      "Test loss: 0.37020462116348407\n",
      "Test accuracy: 0.8037383179798304\n",
      "\n",
      "===================FOLD= 2\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 110s 5s/step - loss: 0.8374 - acc: 0.5836 - val_loss: 0.5017 - val_acc: 0.7322\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 109s 5s/step - loss: nan - acc: 0.2364 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 107s 4s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 108s 4s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 110s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 110s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 107s 4s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 108s 5s/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Train loss: 0.4714555533689873\n",
      "Train accuracy: 0.7738317757009345\n",
      "Test loss: 0.5016635221041991\n",
      "Test accuracy: 0.7322097383858113\n",
      "\n",
      " Train Log Loss Validation=  0.42103829976364127\n",
      " Test Log Loss Validation=  0.452472565990686\n"
     ]
    }
   ],
   "source": [
    "preds=myAngleCV(X_train, X_angle, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "012fc91e-17ff-4163-a32d-79007feba4fc",
    "_uuid": "2e7f1db4b36211939fb9650e3b721ac8db09dda2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']\n",
    "submission['is_iceberg']=preds\n",
    "\n",
    "# 生成一个ndarray，装要插入的值\n",
    "is_iceberg = np.zeros(submission.shape[0])\n",
    "# 按条件修改two\n",
    "for i in range(submission.shape[0]):\n",
    "    if submission['is_iceberg'][i] < 1:\n",
    "        is_iceberg[i] = int(0)\n",
    "    else:\n",
    "        is_iceberg[i] = int(1)\n",
    "# 完成后将two插入dataframe中\n",
    "submission['is_iceberg']=is_iceberg\n",
    "\n",
    "submission.to_csv('sub_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a8d9b1fd</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e7727e</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92a51ffb</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c769ac97</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aee0547d</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>565b28ac</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e04e9775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8e8161d1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4cf4d256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>139e5324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>f156976f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>68a117cc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>d9aa7a56</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9005b143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5f6d3988</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9ad70954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b9087b9e</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a39a1427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>82fbe8ed</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1fae4879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6dd8f13d</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bbad5958</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>54527583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>be8fa29c</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>81a3328f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>8ae30ce6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8395</th>\n",
       "      <td>de27ed88</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8396</th>\n",
       "      <td>66d5196f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>d85f1858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>16dcb33a</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>eca3158e</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>08daeee6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>e9c513ee</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>b1519fa6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>dfc89540</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8404</th>\n",
       "      <td>8fd8c0e9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>45df6347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8406</th>\n",
       "      <td>bf7928d7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>7b587c05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>c2834388</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>146143c3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8410</th>\n",
       "      <td>d59aee00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>cbc0b93b</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>088e2ff7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>673d33cd</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>674b031e</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>43db4207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>156855e1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8417</th>\n",
       "      <td>ac96cfb0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>fe45aef5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>16ee9b50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>5a599eb7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>df30d6dd</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8422</th>\n",
       "      <td>18af95b1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8423</th>\n",
       "      <td>27d788c8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8424 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  is_iceberg\n",
       "0     5941774d         0.0\n",
       "1     4023181e         0.0\n",
       "2     b20200e4         0.0\n",
       "3     e7f018bb         0.0\n",
       "4     4371c8c3         0.0\n",
       "5     a8d9b1fd         0.0\n",
       "6     29e7727e         0.0\n",
       "7     92a51ffb         0.0\n",
       "8     c769ac97         0.0\n",
       "9     aee0547d         0.0\n",
       "10    565b28ac         0.0\n",
       "11    e04e9775         0.0\n",
       "12    8e8161d1         0.0\n",
       "13    4cf4d256         0.0\n",
       "14    139e5324         0.0\n",
       "15    f156976f         0.0\n",
       "16    68a117cc         0.0\n",
       "17    d9aa7a56         0.0\n",
       "18    9005b143         0.0\n",
       "19    5f6d3988         0.0\n",
       "20    9ad70954         0.0\n",
       "21    b9087b9e         0.0\n",
       "22    a39a1427         0.0\n",
       "23    82fbe8ed         0.0\n",
       "24    1fae4879         0.0\n",
       "25    6dd8f13d         0.0\n",
       "26    bbad5958         0.0\n",
       "27    54527583         0.0\n",
       "28    be8fa29c         0.0\n",
       "29    81a3328f         0.0\n",
       "...        ...         ...\n",
       "8394  8ae30ce6         0.0\n",
       "8395  de27ed88         0.0\n",
       "8396  66d5196f         0.0\n",
       "8397  d85f1858         0.0\n",
       "8398  16dcb33a         0.0\n",
       "8399  eca3158e         0.0\n",
       "8400  08daeee6         0.0\n",
       "8401  e9c513ee         0.0\n",
       "8402  b1519fa6         0.0\n",
       "8403  dfc89540         0.0\n",
       "8404  8fd8c0e9         0.0\n",
       "8405  45df6347         0.0\n",
       "8406  bf7928d7         0.0\n",
       "8407  7b587c05         0.0\n",
       "8408  c2834388         0.0\n",
       "8409  146143c3         0.0\n",
       "8410  d59aee00         0.0\n",
       "8411  cbc0b93b         0.0\n",
       "8412  088e2ff7         0.0\n",
       "8413  673d33cd         0.0\n",
       "8414  674b031e         0.0\n",
       "8415  43db4207         0.0\n",
       "8416  156855e1         0.0\n",
       "8417  ac96cfb0         0.0\n",
       "8418  fe45aef5         0.0\n",
       "8419  16ee9b50         0.0\n",
       "8420  5a599eb7         0.0\n",
       "8421  df30d6dd         0.0\n",
       "8422  18af95b1         0.0\n",
       "8423  27d788c8         0.0\n",
       "\n",
       "[8424 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

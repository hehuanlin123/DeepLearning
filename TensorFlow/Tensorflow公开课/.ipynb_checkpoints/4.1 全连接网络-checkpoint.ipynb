{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "INPUT_NODE = 784 #28*28像素点\n",
    "OUTPUT_NODE = 10 #输出10个数\n",
    "LAYER1_NODE = 500 #隐藏层节点个数\n",
    "\n",
    "def get_weight(shape,regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "def forward(x,regularizer):\n",
    "    w1 = get_weight([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "    y1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    w2 = get_weight([LAYER1_NODE,OUTPUT_NODE,],regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "    y = tf.nn.relu(tf.matmul(y1,w2)+b2)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-1fbd92da2e63>:55: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "/Users/he; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1fbd92da2e63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1fbd92da2e63>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/he/Desktop/hehuanlin/datasets/mnist/data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.pyc\u001b[0m in \u001b[0;36mread_data_sets\u001b[0;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n\u001b[0;32m--> 260\u001b[0;31m                                    source_url + TRAIN_IMAGES)\n\u001b[0m\u001b[1;32m    261\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.pyc\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.pyc\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, work_directory, source_url)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \"\"\"\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[0;34m(dirname)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m   \u001b[0mrecursive_create_dir_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.pyc\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \"\"\"\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /Users/he; Permission denied"
     ]
    }
   ],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "#import mnist_forward\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZER = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = \"/Users/szkfzx/datasets/mnist/model\"\n",
    "MODEL_NAME = \"mnist_model\"\n",
    "\n",
    "def backward(mnist):\n",
    "    \n",
    "    x = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "    y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "    y = forward(x,REGULARIZER)\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,\n",
    "                                               global_step,\n",
    "                                               mnist.train.num_examples / BATCH_SIZE,\n",
    "                                               LEARNING_RATE_DECAY,\n",
    "                                               staircase=True)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,\n",
    "                                                                           global_step=global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            if i % 1000 == 0:\n",
    "                print \"After %d training step(s),loss on training batch is %g\" % (step,loss_value)\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "                \n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"/Users/szkfzx/datasets/mnist/data/\",one_hot=True)\n",
    "    backward(mnist)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from /Users/he/Desktop/hehuanlin/datasets/mnist/model/mnist_model-49001\n",
      "After 49001 training step(s),test accuracy = 0.9799\n"
     ]
    }
   ],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "# import mnist_forward\n",
    "# import mnist_backward\n",
    "\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "def test(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "        y = forward(x,None)\n",
    "    \n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score = sess.run(accuracy,feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "                    print \"After %s training step(s),test accuracy = %g\" % (global_step,accuracy_score)\n",
    "                else:\n",
    "                    print \"No checkpoint file found\"\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "            break\n",
    "            \n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"/Users/he/Desktop/hehuanlin/datasets/mnist/data/\",one_hot=True)\n",
    "    test(mnist)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def restore_model(testPicArr):\n",
    "    #创建一个默认图，在该图中执行以下操作（多数操作和train中一样，就不再重复解释，大家对照学习即可）\n",
    "    with tf.Graph().as_default() as tg:\n",
    "        x = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        y = forward(x,None)\n",
    "        preValue = tf.argmax(y,1)#得到概率最大的预测值\n",
    "        \n",
    "        #实现滑动平均模型，参数MOVING_AVERAGE_DECAY用于控制模型更新的速度，训练过程中会对每一个变量维护一个影子变量\n",
    "        #这个影子变量就是相应变量的初始值，每次变量更新时，影子变量就会随之更新\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #通过checkpoint文件定位到最新保存的模型\n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                \n",
    "                preValue = sess.run(preValue,feed_dict={x:testPicArr})\n",
    "                return preValue\n",
    "            else:\n",
    "                print \"No checkpoint file found\"\n",
    "                return -1\n",
    "\n",
    "#预处理函数，包括resize、转变灰度图、二值化操作 \n",
    "def pre_dic(picName):\n",
    "    img = Image.open(picName)\n",
    "    reIm = img.resize((28,28),Image.ANTIALIAS)\n",
    "    im_arr = np.array(reIm.convert('L'))\n",
    "    threshold = 50#设定合理的阈值\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            im_arr[i][j] = 255 - im_arr[i][j]\n",
    "            if(im_arr[i][j] < threshold):\n",
    "                im_arr[i][j] = 0\n",
    "            else:\n",
    "                im_arr[i][j] = 255\n",
    "                \n",
    "    nm_arr = im_arr.reshape([1,784])\n",
    "    nm_arr = nm_arr.astype(np.float32)\n",
    "    img_ready = np.multiply(nm_arr,1.0/255.0)\n",
    "    \n",
    "    return img_ready\n",
    "            \n",
    "def application():\n",
    "    testNum = input(\"input the number of test pictures:\") \n",
    "    for i in range(testNum):\n",
    "        testPic = raw_input(\"the path of test picture:\")\n",
    "        testPicArr = pre_pic(testPic)\n",
    "        preValue = restore_model(testPicArr)\n",
    "        print \"The prediction number is:\", preValue\n",
    "        \n",
    "def main():\n",
    "    application()\n",
    "    \n",
    "if __name__ == '__main()':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_train_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_train_jpg_60000/'\n",
    "label_train_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_train_jpg_60000.txt'\n",
    "tfRecord_train = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_train.tfrecords'\n",
    "image_test_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_test_jpg_10000/'\n",
    "lebel_train_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_test_jpg_10000.txt'\n",
    "tfRecord_test = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_test.tfrecords'\n",
    "data_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/'\n",
    "resize_height = 28\n",
    "resize_width = 28\n",
    "\n",
    "def write_tfRecord(tfRecordName,image_path,label_path):\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordName)\n",
    "    num_pic = 0\n",
    "    f = open(lebel_path,'r')\n",
    "    f.close()\n",
    "    for content in contents:\n",
    "        value = content.split()\n",
    "        img_path = image_path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img_raw = img.tobytes()\n",
    "        labels = [0] * 10\n",
    "        lebels[int(value[1])] = 1\n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'img_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n",
    "            'label':tf.train.Feature(int64_list=tf.train.Int64List(value=lebels))\n",
    "        }))\n",
    "        \n",
    "        writer.write(example.SerializeToString())\n",
    "        num_pic += 1\n",
    "        print \"the number of picture:\",num_pic\n",
    "        \n",
    "    writer.close()\n",
    "    print \"write tfrecord successful\"\n",
    "    \n",
    "def generate_tfRecord():\n",
    "    isExists = os.path.exists(data_path)\n",
    "    if not isExists:\n",
    "        os.makedirs(data_path)\n",
    "        print \"The directory was created successfully\"\n",
    "    else:\n",
    "        print \"directory already exists\"\n",
    "    wirte_tfRecord(tfRecord_train,image_train_path,label_train_path)\n",
    "    wirte_tfRecord(tfRecord_test,image_test_path,label_test_path)\n",
    "    \n",
    "def read_tfRecord(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])\n",
    "    reader = tf.TFRecorderReader()\n",
    "    _,serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                      features={\n",
    "                                          'label':tf.FixedLenFeature([10],tf.int64),\n",
    "                                          'img_raw':tf.FixedLenFeature([],tf.string)\n",
    "                                      })\n",
    "    img = tf.decode_raw(features['img_raw'],tf.uint8)\n",
    "    img.set_shape([784])\n",
    "    img = tf.cast(img,tf.float32)*(1./255)\n",
    "    label = tf.cast(features['label'],tf.float32)\n",
    "    return img,label\n",
    "\n",
    "def get_tfrecord(num,isTrain=True):\n",
    "    if isTrain:\n",
    "        tfRecord_path = tfRecord_train\n",
    "    else:\n",
    "        tfRecord_path = tfRecord_test\n",
    "    img,lebel = read_tfRecord(tfRecord_path)\n",
    "    img_batch,label_batch = tf.train.shuffle_batch([img,label],\n",
    "                                                   batch_size=num,\n",
    "                                                   num_threads=2,\n",
    "                                                   capacity=1000,\n",
    "                                                   min_after_dequeue=700)\n",
    "    return img_batch,label_batch\n",
    "\n",
    "def main():\n",
    "    generate_tfRecord()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

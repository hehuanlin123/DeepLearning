{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]]\n",
      "Y:\n",
      "[[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n",
      "w1:\n",
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 0 training step(s),loss on all data is 5.13118\n",
      "After 500 training step(s),loss on all data is 0.429111\n",
      "After 1000 training step(s),loss on all data is 0.409789\n",
      "After 1500 training step(s),loss on all data is 0.399923\n",
      "After 2000 training step(s),loss on all data is 0.394146\n",
      "After 2500 training step(s),loss on all data is 0.390597\n",
      "After 3000 training step(s),loss on all data is 0.388336\n",
      "After 3500 training step(s),loss on all data is 0.386855\n",
      "After 4000 training step(s),loss on all data is 0.385863\n",
      "After 4500 training step(s),loss on all data is 0.385186\n",
      "After 5000 training step(s),loss on all data is 0.384719\n",
      "After 5500 training step(s),loss on all data is 0.384391\n",
      "After 6000 training step(s),loss on all data is 0.38416\n",
      "After 6500 training step(s),loss on all data is 0.383995\n",
      "After 7000 training step(s),loss on all data is 0.383877\n",
      "After 7500 training step(s),loss on all data is 0.383791\n",
      "After 8000 training step(s),loss on all data is 0.383729\n",
      "After 8500 training step(s),loss on all data is 0.383684\n",
      "After 9000 training step(s),loss on all data is 0.383652\n",
      "After 9500 training step(s),loss on all data is 0.383628\n",
      "After 10000 training step(s),loss on all data is 0.38361\n",
      "After 10500 training step(s),loss on all data is 0.383597\n",
      "After 11000 training step(s),loss on all data is 0.383587\n",
      "After 11500 training step(s),loss on all data is 0.38358\n",
      "After 12000 training step(s),loss on all data is 0.383575\n",
      "After 12500 training step(s),loss on all data is 0.383571\n",
      "After 13000 training step(s),loss on all data is 0.383568\n",
      "After 13500 training step(s),loss on all data is 0.383566\n",
      "After 14000 training step(s),loss on all data is 0.383565\n",
      "After 14500 training step(s),loss on all data is 0.383564\n",
      "After 15000 training step(s),loss on all data is 0.383563\n",
      "After 15500 training step(s),loss on all data is 0.383562\n",
      "After 16000 training step(s),loss on all data is 0.383562\n",
      "After 16500 training step(s),loss on all data is 0.383561\n",
      "After 17000 training step(s),loss on all data is 0.383561\n",
      "After 17500 training step(s),loss on all data is 0.383561\n",
      "After 18000 training step(s),loss on all data is 0.383561\n",
      "After 18500 training step(s),loss on all data is 0.383561\n",
      "After 19000 training step(s),loss on all data is 0.383561\n",
      "After 19500 training step(s),loss on all data is 0.383561\n",
      "\n",
      "\n",
      "w1:\n",
      "[[-0.69032323  0.804551    0.09699076]\n",
      " [-2.343718   -0.10306353  0.58510697]]\n",
      "w2:\n",
      "[[-0.09008674]\n",
      " [ 0.80441564]\n",
      " [-0.05047939]]\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#0、导入模块，常量定义，生成模拟数据集\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "seed = 23455\n",
    "\n",
    "#基于seed生成随机数\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "#随机数返回32行2列的矩阵，表示32组体积和重量，作为输入数据集\n",
    "X = rng.rand(32,2)\n",
    "\n",
    "#从X这个32行2列的矩阵中，取出一行，判断如果和小于1，给Y赋值1，如果和不小于1，给Y赋值0\n",
    "#作为输入数据集的标签（正确答案）\n",
    "Y = [[int(x0 + x1 < 1)] for (x0,x1) in X]\n",
    "print \"X:\\n\",X\n",
    "print \"Y:\\n\",Y\n",
    "\n",
    "#1、定义神经网络的输入、参数和输出，定义前向传播过程\n",
    "x = tf.placeholder(tf.float32,shape=(None,2))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1],stddev=1,seed=1))\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#2、定义损失函数及反向传播方法\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.MomentumOptimizer(0.001).minimize(loss)\n",
    "#train_step = tf.train.AdamDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#3、生成会话，训练STEPS轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #输出目前（未经训练）的参数取值\n",
    "    print \"w1:\\n\",sess.run(w1)\n",
    "    print \"w2:\\n\",sess.run(w2)\n",
    "    print \"\\n\"\n",
    "\n",
    "    #训练模型\n",
    "    STEPS = 20000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss,feed_dict={x:X,y_:Y})\n",
    "            print \"After %d training step(s),loss on all data is %g\" %(i,total_loss)\n",
    "\n",
    "    #输出训练后的参数取值\n",
    "    print \"\\n\"\n",
    "    print \"w1:\\n\",sess.run(w1)\n",
    "    print \"w2:\\n\",sess.run(w2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

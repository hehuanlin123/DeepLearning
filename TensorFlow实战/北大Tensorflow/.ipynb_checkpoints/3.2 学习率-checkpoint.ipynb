{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 steps: w is 2.600000, loss is 12.959999.\n",
      "After 1 steps: w is 1.160000, loss is 4.665599.\n",
      "After 2 steps: w is 0.296000, loss is 1.679616.\n",
      "After 3 steps: w is -0.222400, loss is 0.604662.\n",
      "After 4 steps: w is -0.533440, loss is 0.217678.\n",
      "After 5 steps: w is -0.720064, loss is 0.078364.\n",
      "After 6 steps: w is -0.832038, loss is 0.028211.\n",
      "After 7 steps: w is -0.899223, loss is 0.010156.\n",
      "After 8 steps: w is -0.939534, loss is 0.003656.\n",
      "After 9 steps: w is -0.963720, loss is 0.001316.\n",
      "After 10 steps: w is -0.978232, loss is 0.000474.\n",
      "After 11 steps: w is -0.986939, loss is 0.000171.\n",
      "After 12 steps: w is -0.992164, loss is 0.000061.\n",
      "After 13 steps: w is -0.995298, loss is 0.000022.\n",
      "After 14 steps: w is -0.997179, loss is 0.000008.\n",
      "After 15 steps: w is -0.998307, loss is 0.000003.\n",
      "After 16 steps: w is -0.998984, loss is 0.000001.\n",
      "After 17 steps: w is -0.999391, loss is 0.000000.\n",
      "After 18 steps: w is -0.999634, loss is 0.000000.\n",
      "After 19 steps: w is -0.999781, loss is 0.000000.\n",
      "After 20 steps: w is -0.999868, loss is 0.000000.\n",
      "After 21 steps: w is -0.999921, loss is 0.000000.\n",
      "After 22 steps: w is -0.999953, loss is 0.000000.\n",
      "After 23 steps: w is -0.999972, loss is 0.000000.\n",
      "After 24 steps: w is -0.999983, loss is 0.000000.\n",
      "After 25 steps: w is -0.999990, loss is 0.000000.\n",
      "After 26 steps: w is -0.999994, loss is 0.000000.\n",
      "After 27 steps: w is -0.999996, loss is 0.000000.\n",
      "After 28 steps: w is -0.999998, loss is 0.000000.\n",
      "After 29 steps: w is -0.999999, loss is 0.000000.\n",
      "After 30 steps: w is -0.999999, loss is 0.000000.\n",
      "After 31 steps: w is -1.000000, loss is 0.000000.\n",
      "After 32 steps: w is -1.000000, loss is 0.000000.\n",
      "After 33 steps: w is -1.000000, loss is 0.000000.\n",
      "After 34 steps: w is -1.000000, loss is 0.000000.\n",
      "After 35 steps: w is -1.000000, loss is 0.000000.\n",
      "After 36 steps: w is -1.000000, loss is 0.000000.\n",
      "After 37 steps: w is -1.000000, loss is 0.000000.\n",
      "After 38 steps: w is -1.000000, loss is 0.000000.\n",
      "After 39 steps: w is -1.000000, loss is 0.000000.\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#设损失函数 loss=(w+1)^2，令w初值是常数5。反向传播就是求最优化w，即求最小loss对应的w值\n",
    "import tensorflow as tf\n",
    "#定义待优化参数w初值赋5\n",
    "w = tf.Variable(tf.constant(5,dtype=tf.float32))\n",
    "#定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "#定义反向传播方法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "#生成会话，训练40轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print \"After %s steps: w is %f, loss is %f.\"%(i,w_val,loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 steps: global_step is 0.099000, w is 1.000000, learning_rate is 7.800000, loss is 77.440002.\n",
      "After 1 steps: global_step is 0.098010, w is 2.000000, learning_rate is 6.057600, loss is 49.809719.\n",
      "After 2 steps: global_step is 0.097030, w is 3.000000, learning_rate is 4.674169, loss is 32.196194.\n",
      "After 3 steps: global_step is 0.096060, w is 4.000000, learning_rate is 3.573041, loss is 20.912704.\n",
      "After 4 steps: global_step is 0.095099, w is 5.000000, learning_rate is 2.694472, loss is 13.649122.\n",
      "After 5 steps: global_step is 0.094148, w is 6.000000, learning_rate is 1.991791, loss is 8.950810.\n",
      "After 6 steps: global_step is 0.093207, w is 7.000000, learning_rate is 1.428448, loss is 5.897361.\n",
      "After 7 steps: global_step is 0.092274, w is 8.000000, learning_rate is 0.975754, loss is 3.903603.\n",
      "After 8 steps: global_step is 0.091352, w is 9.000000, learning_rate is 0.611130, loss is 2.595741.\n",
      "After 9 steps: global_step is 0.090438, w is 10.000000, learning_rate is 0.316771, loss is 1.733887.\n",
      "After 10 steps: global_step is 0.089534, w is 11.000000, learning_rate is 0.078598, loss is 1.163374.\n",
      "After 11 steps: global_step is 0.088638, w is 12.000000, learning_rate is -0.114544, loss is 0.784033.\n",
      "After 12 steps: global_step is 0.087752, w is 13.000000, learning_rate is -0.271515, loss is 0.530691.\n",
      "After 13 steps: global_step is 0.086875, w is 14.000000, learning_rate is -0.399367, loss is 0.360760.\n",
      "After 14 steps: global_step is 0.086006, w is 15.000000, learning_rate is -0.503727, loss is 0.246287.\n",
      "After 15 steps: global_step is 0.085146, w is 16.000000, learning_rate is -0.589091, loss is 0.168846.\n",
      "After 16 steps: global_step is 0.084294, w is 17.000000, learning_rate is -0.659066, loss is 0.116236.\n",
      "After 17 steps: global_step is 0.083451, w is 18.000000, learning_rate is -0.716543, loss is 0.080348.\n",
      "After 18 steps: global_step is 0.082617, w is 19.000000, learning_rate is -0.763853, loss is 0.055765.\n",
      "After 19 steps: global_step is 0.081791, w is 20.000000, learning_rate is -0.802872, loss is 0.038859.\n",
      "After 20 steps: global_step is 0.080973, w is 21.000000, learning_rate is -0.835119, loss is 0.027186.\n",
      "After 21 steps: global_step is 0.080163, w is 22.000000, learning_rate is -0.861821, loss is 0.019094.\n",
      "After 22 steps: global_step is 0.079361, w is 23.000000, learning_rate is -0.883974, loss is 0.013462.\n",
      "After 23 steps: global_step is 0.078568, w is 24.000000, learning_rate is -0.902390, loss is 0.009528.\n",
      "After 24 steps: global_step is 0.077782, w is 25.000000, learning_rate is -0.917728, loss is 0.006769.\n",
      "After 25 steps: global_step is 0.077004, w is 26.000000, learning_rate is -0.930527, loss is 0.004827.\n",
      "After 26 steps: global_step is 0.076234, w is 27.000000, learning_rate is -0.941226, loss is 0.003454.\n",
      "After 27 steps: global_step is 0.075472, w is 28.000000, learning_rate is -0.950187, loss is 0.002481.\n",
      "After 28 steps: global_step is 0.074717, w is 29.000000, learning_rate is -0.957706, loss is 0.001789.\n",
      "After 29 steps: global_step is 0.073970, w is 30.000000, learning_rate is -0.964026, loss is 0.001294.\n",
      "After 30 steps: global_step is 0.073230, w is 31.000000, learning_rate is -0.969348, loss is 0.000940.\n",
      "After 31 steps: global_step is 0.072498, w is 32.000000, learning_rate is -0.973838, loss is 0.000684.\n",
      "After 32 steps: global_step is 0.071773, w is 33.000000, learning_rate is -0.977631, loss is 0.000500.\n",
      "After 33 steps: global_step is 0.071055, w is 34.000000, learning_rate is -0.980842, loss is 0.000367.\n",
      "After 34 steps: global_step is 0.070345, w is 35.000000, learning_rate is -0.983565, loss is 0.000270.\n",
      "After 35 steps: global_step is 0.069641, w is 36.000000, learning_rate is -0.985877, loss is 0.000199.\n",
      "After 36 steps: global_step is 0.068945, w is 37.000000, learning_rate is -0.987844, loss is 0.000148.\n",
      "After 37 steps: global_step is 0.068255, w is 38.000000, learning_rate is -0.989520, loss is 0.000110.\n",
      "After 38 steps: global_step is 0.067573, w is 39.000000, learning_rate is -0.990951, loss is 0.000082.\n",
      "After 39 steps: global_step is 0.066897, w is 40.000000, learning_rate is -0.992174, loss is 0.000061.\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "#设损失函数 loss=(w+1)^2，令w初值是常数5。反向传播就是求最优化w，即求最小loss对应的w值\n",
    "#使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度\n",
    "import tensorflow as tf\n",
    "\n",
    "LEARNING_RATE_BASE = 0.1 #最初学习率\n",
    "LEARNING_RATE_DECAY = 0.99 #学习率衰减率\n",
    "LEARNING_RATE_STEP = 1 #喂入多少轮BATCH_SIZE后，更新一次学习率，一般设为：总样本数/BATCH_SIZE\n",
    "\n",
    "#运行了几轮BATCH_SIZE的计数器，初值给0，设为不被训练\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "\n",
    "#定义指数下降学习率\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,global_step,LEARNING_RATE_STEP,\n",
    "                                           LEARNING_RATE_DECAY,staircase=True)\n",
    "#定义待优化参数w初值赋5\n",
    "w = tf.Variable(tf.constant(5,dtype=tf.float32))\n",
    "#定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "#定义反向传播方法\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,\n",
    "                                                                       global_step=global_step)\n",
    "#生成会话，训练40轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print \"After %s steps: global_step is %f, w is %f, learning_rate is %f, loss is %f.\"%(i,learning_rate_val,global_step_val,w_val,loss_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "INPUT_NODE = 784 #28*28像素点\n",
    "OUTPUT_NODE = 10 #输出10个数\n",
    "LAYER1_NODE = 500 #隐藏层节点个数\n",
    "\n",
    "def get_weight(shape,regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "def forward(x,regularizer):\n",
    "    w1 = get_weight([INPUT_NODE,LAYER1_NODE],regularizer)\n",
    "    b1 = get_bias([LAYER1_NODE])\n",
    "    y1 = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "    \n",
    "    w2 = get_weight([LAYER1_NODE,OUTPUT_NODE,],regularizer)\n",
    "    b2 = get_bias([OUTPUT_NODE])\n",
    "    y = tf.nn.relu(tf.matmul(y1,w2)+b2)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s),loss on training batch is 2.6901\n",
      "After 1001 training step(s),loss on training batch is 0.319693\n",
      "After 2001 training step(s),loss on training batch is 0.250634\n",
      "After 3001 training step(s),loss on training batch is 0.275783\n",
      "After 4001 training step(s),loss on training batch is 0.266125\n",
      "After 5001 training step(s),loss on training batch is 0.223072\n",
      "After 6001 training step(s),loss on training batch is 0.2379\n",
      "After 7001 training step(s),loss on training batch is 0.203823\n",
      "After 8001 training step(s),loss on training batch is 0.175558\n",
      "After 9001 training step(s),loss on training batch is 0.206938\n",
      "After 10001 training step(s),loss on training batch is 0.171093\n",
      "After 11001 training step(s),loss on training batch is 0.162006\n",
      "After 12001 training step(s),loss on training batch is 0.169069\n",
      "After 13001 training step(s),loss on training batch is 0.158871\n",
      "After 14001 training step(s),loss on training batch is 0.150419\n",
      "After 15001 training step(s),loss on training batch is 0.160066\n",
      "After 16001 training step(s),loss on training batch is 0.16533\n",
      "After 17001 training step(s),loss on training batch is 0.154572\n",
      "After 18001 training step(s),loss on training batch is 0.147472\n",
      "After 19001 training step(s),loss on training batch is 0.148468\n",
      "After 20001 training step(s),loss on training batch is 0.148494\n",
      "After 21001 training step(s),loss on training batch is 0.149809\n",
      "After 22001 training step(s),loss on training batch is 0.146689\n",
      "After 23001 training step(s),loss on training batch is 0.144038\n",
      "After 24001 training step(s),loss on training batch is 0.144812\n",
      "After 25001 training step(s),loss on training batch is 0.142948\n",
      "After 26001 training step(s),loss on training batch is 0.134325\n",
      "After 27001 training step(s),loss on training batch is 0.145071\n",
      "After 28001 training step(s),loss on training batch is 0.151947\n",
      "After 29001 training step(s),loss on training batch is 0.133011\n",
      "After 30001 training step(s),loss on training batch is 0.132565\n",
      "After 31001 training step(s),loss on training batch is 0.132626\n",
      "After 32001 training step(s),loss on training batch is 0.137947\n",
      "After 33001 training step(s),loss on training batch is 0.129296\n",
      "After 34001 training step(s),loss on training batch is 0.129742\n",
      "After 35001 training step(s),loss on training batch is 0.130497\n",
      "After 36001 training step(s),loss on training batch is 0.140401\n",
      "After 37001 training step(s),loss on training batch is 0.134763\n",
      "After 38001 training step(s),loss on training batch is 0.133745\n",
      "After 39001 training step(s),loss on training batch is 0.132906\n",
      "After 40001 training step(s),loss on training batch is 0.132271\n",
      "After 41001 training step(s),loss on training batch is 0.133139\n",
      "After 42001 training step(s),loss on training batch is 0.130144\n",
      "After 43001 training step(s),loss on training batch is 0.128093\n",
      "After 44001 training step(s),loss on training batch is 0.131302\n",
      "After 45001 training step(s),loss on training batch is 0.128604\n",
      "After 46001 training step(s),loss on training batch is 0.128663\n",
      "After 47001 training step(s),loss on training batch is 0.123189\n",
      "After 48001 training step(s),loss on training batch is 0.127612\n",
      "After 49001 training step(s),loss on training batch is 0.126052\n"
     ]
    }
   ],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "#import mnist_forward\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZER = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = \"/Users/he/Desktop/hehuanlin/datasets/mnist/model\"\n",
    "MODEL_NAME = \"mnist_model\"\n",
    "\n",
    "def backward(mnist):\n",
    "    \n",
    "    x = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "    y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "    y = forward(x,REGULARIZER)\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "    loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE,\n",
    "                                               global_step,\n",
    "                                               mnist.train.num_examples / BATCH_SIZE,\n",
    "                                               LEARNING_RATE_DECAY,\n",
    "                                               staircase=True)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,\n",
    "                                                                           global_step=global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            if i % 1000 == 0:\n",
    "                print \"After %d training step(s),loss on training batch is %g\" % (step,loss_value)\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "                \n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"/Users/he/Desktop/hehuanlin/datasets/mnist/data/\",one_hot=True)\n",
    "    backward(mnist)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/he/Desktop/hehuanlin/datasets/mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from /Users/he/Desktop/hehuanlin/datasets/mnist/model/mnist_model-49001\n",
      "After 49001 training step(s),test accuracy = 0.9799\n"
     ]
    }
   ],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "# import mnist_forward\n",
    "# import mnist_backward\n",
    "\n",
    "TEST_INTERVAL_SECS = 5\n",
    "\n",
    "def test(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "        y = forward(x,None)\n",
    "    \n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score = sess.run(accuracy,feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "                    print \"After %s training step(s),test accuracy = %g\" % (global_step,accuracy_score)\n",
    "                else:\n",
    "                    print \"No checkpoint file found\"\n",
    "                    return\n",
    "            time.sleep(TEST_INTERVAL_SECS)\n",
    "            break\n",
    "            \n",
    "def main():\n",
    "    mnist = input_data.read_data_sets(\"/Users/he/Desktop/hehuanlin/datasets/mnist/data/\",one_hot=True)\n",
    "    test(mnist)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def restore_model(testPicArr):\n",
    "    #创建一个默认图，在该图中执行以下操作（多数操作和train中一样，就不再重复解释，大家对照学习即可）\n",
    "    with tf.Graph().as_default() as tg:\n",
    "        x = tf.placeholder(tf.float32,[None,INPUT_NODE])\n",
    "        y = forward(x,None)\n",
    "        preValue = tf.argmax(y,1)#得到概率最大的预测值\n",
    "        \n",
    "        #实现滑动平均模型，参数MOVING_AVERAGE_DECAY用于控制模型更新的速度，训练过程中会对每一个变量维护一个影子变量\n",
    "        #这个影子变量就是相应变量的初始值，每次变量更新时，影子变量就会随之更新\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #通过checkpoint文件定位到最新保存的模型\n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                \n",
    "                preValue = sess.run(preValue,feed_dict={x:testPicArr})\n",
    "                return preValue\n",
    "            else:\n",
    "                print \"No checkpoint file found\"\n",
    "                return -1\n",
    "\n",
    "#预处理函数，包括resize、转变灰度图、二值化操作 \n",
    "def pre_dic(picName):\n",
    "    img = Image.open(picName)\n",
    "    reIm = img.resize((28,28),Image.ANTIALIAS)\n",
    "    im_arr = np.array(reIm.convert('L'))\n",
    "    threshold = 50#设定合理的阈值\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            im_arr[i][j] = 255 - im_arr[i][j]\n",
    "            if(im_arr[i][j] < threshold):\n",
    "                im_arr[i][j] = 0\n",
    "            else:\n",
    "                im_arr[i][j] = 255\n",
    "                \n",
    "    nm_arr = im_arr.reshape([1,784])\n",
    "    nm_arr = nm_arr.astype(np.float32)\n",
    "    img_ready = np.multiply(nm_arr,1.0/255.0)\n",
    "    \n",
    "    return img_ready\n",
    "            \n",
    "def application():\n",
    "    testNum = input(\"input the number of test pictures:\") \n",
    "    for i in range(testNum):\n",
    "        testPic = raw_input(\"the path of test picture:\")\n",
    "        testPicArr = pre_pic(testPic)\n",
    "        preValue = restore_model(testPicArr)\n",
    "        print \"The prediction number is:\", preValue\n",
    "        \n",
    "def main():\n",
    "    application()\n",
    "    \n",
    "if __name__ == '__main()':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conding:utf-8\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_train_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_train_jpg_60000/'\n",
    "label_train_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_train_jpg_60000.txt'\n",
    "tfRecord_train = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_train.tfrecords'\n",
    "image_test_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_test_jpg_10000/'\n",
    "lebel_train_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_data_jpg/mnist_test_jpg_10000.txt'\n",
    "tfRecord_test = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/mnist_test.tfrecords'\n",
    "data_path = '/Users/he/Desktop/hehuanlin/datasets/mnist/data/'\n",
    "resize_height = 28\n",
    "resize_width = 28\n",
    "\n",
    "def write_tfRecord(tfRecordName,image_path,label_path):\n",
    "    writer = tf.python_io.TFRecordWriter(tfRecordName)\n",
    "    num_pic = 0\n",
    "    f = open(lebel_path,'r')\n",
    "    f.close()\n",
    "    for content in contents:\n",
    "        value = content.split()\n",
    "        img_path = image_path + value[0]\n",
    "        img = Image.open(img_path)\n",
    "        img_raw = img.tobytes()\n",
    "        labels = [0] * 10\n",
    "        lebels[int(value[1])] = 1\n",
    "        \n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'img_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n",
    "            'label':tf.train.Feature(int64_list=tf.train.Int64List(value=lebels))\n",
    "        }))\n",
    "        \n",
    "        writer.write(example.SerializeToString())\n",
    "        num_pic += 1\n",
    "        print \"the number of picture:\",num_pic\n",
    "        \n",
    "    writer.close()\n",
    "    print \"write tfrecord successful\"\n",
    "    \n",
    "def generate_tfRecord():\n",
    "    isExists = os.path.exists(data_path)\n",
    "    if not isExists:\n",
    "        os.makedirs(data_path)\n",
    "        print \"The directory was created successfully\"\n",
    "    else:\n",
    "        print \"directory already exists\"\n",
    "    wirte_tfRecord(tfRecord_train,image_train_path,label_train_path)\n",
    "    wirte_tfRecord(tfRecord_test,image_test_path,label_test_path)\n",
    "    \n",
    "def read_tfRecord(tfRecord_path):\n",
    "    filename_queue = tf.train.string_input_producer([tfRecord_path])\n",
    "    reader = tf.TFRecorderReader()\n",
    "    _,serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                      features={\n",
    "                                          'label':tf.FixedLenFeature([10],tf.int64),\n",
    "                                          'img_raw':tf.FixedLenFeature([],tf.string)\n",
    "                                      })\n",
    "    img = tf.decode_raw(features['img_raw'],tf.uint8)\n",
    "    img.set_shape([784])\n",
    "    img = tf.cast(img,tf.float32)*(1./255)\n",
    "    label = tf.cast(features['label'],tf.float32)\n",
    "    return img,label\n",
    "\n",
    "def get_tfrecord(num,isTrain=True):\n",
    "    if isTrain:\n",
    "        tfRecord_path = tfRecord_train\n",
    "    else:\n",
    "        tfRecord_path = tfRecord_test\n",
    "    img,lebel = read_tfRecord(tfRecord_path)\n",
    "    img_batch,label_batch = tf.train.shuffle_batch([img,label],\n",
    "                                                   batch_size=num,\n",
    "                                                   num_threads=2,\n",
    "                                                   capacity=1000,\n",
    "                                                   min_after_dequeue=700)\n",
    "    return img_batch,label_batch\n",
    "\n",
    "def main():\n",
    "    generate_tfRecord()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tfpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

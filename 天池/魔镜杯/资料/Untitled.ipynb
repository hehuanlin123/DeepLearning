{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szkfzx/anaconda2/envs/python36/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 6435)\n",
      "0 fold...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\ttraining's multi_logloss: 2.39741\tvalid_1's multi_logloss: 2.40487\n",
      "[10]\ttraining's multi_logloss: 2.32166\tvalid_1's multi_logloss: 2.3341\n",
      "[15]\ttraining's multi_logloss: 2.26722\tvalid_1's multi_logloss: 2.28377\n",
      "[20]\ttraining's multi_logloss: 2.22374\tvalid_1's multi_logloss: 2.24401\n",
      "[25]\ttraining's multi_logloss: 2.19036\tvalid_1's multi_logloss: 2.21413\n",
      "[30]\ttraining's multi_logloss: 2.16317\tvalid_1's multi_logloss: 2.19024\n",
      "[35]\ttraining's multi_logloss: 2.14146\tvalid_1's multi_logloss: 2.17189\n",
      "[40]\ttraining's multi_logloss: 2.1227\tvalid_1's multi_logloss: 2.15643\n",
      "[45]\ttraining's multi_logloss: 2.10702\tvalid_1's multi_logloss: 2.14399\n",
      "[50]\ttraining's multi_logloss: 2.09391\tvalid_1's multi_logloss: 2.1342\n",
      "[55]\ttraining's multi_logloss: 2.0825\tvalid_1's multi_logloss: 2.12617\n",
      "[60]\ttraining's multi_logloss: 2.07183\tvalid_1's multi_logloss: 2.11882\n",
      "[65]\ttraining's multi_logloss: 2.06326\tvalid_1's multi_logloss: 2.11349\n",
      "[70]\ttraining's multi_logloss: 2.05498\tvalid_1's multi_logloss: 2.10852\n",
      "[75]\ttraining's multi_logloss: 2.04717\tvalid_1's multi_logloss: 2.10392\n",
      "[80]\ttraining's multi_logloss: 2.04067\tvalid_1's multi_logloss: 2.10069\n",
      "[85]\ttraining's multi_logloss: 2.0339\tvalid_1's multi_logloss: 2.09714\n",
      "[90]\ttraining's multi_logloss: 2.02799\tvalid_1's multi_logloss: 2.09446\n",
      "[95]\ttraining's multi_logloss: 2.02241\tvalid_1's multi_logloss: 2.09207\n",
      "[100]\ttraining's multi_logloss: 2.01708\tvalid_1's multi_logloss: 2.09001\n",
      "[105]\ttraining's multi_logloss: 2.01165\tvalid_1's multi_logloss: 2.08769\n",
      "[110]\ttraining's multi_logloss: 2.00686\tvalid_1's multi_logloss: 2.08599\n",
      "[115]\ttraining's multi_logloss: 2.00238\tvalid_1's multi_logloss: 2.08456\n",
      "[120]\ttraining's multi_logloss: 1.99809\tvalid_1's multi_logloss: 2.08329\n",
      "[125]\ttraining's multi_logloss: 1.99413\tvalid_1's multi_logloss: 2.0823\n",
      "[130]\ttraining's multi_logloss: 1.99005\tvalid_1's multi_logloss: 2.08115\n",
      "[135]\ttraining's multi_logloss: 1.98643\tvalid_1's multi_logloss: 2.08036\n",
      "[140]\ttraining's multi_logloss: 1.98255\tvalid_1's multi_logloss: 2.07927\n",
      "[145]\ttraining's multi_logloss: 1.97892\tvalid_1's multi_logloss: 2.07843\n",
      "[150]\ttraining's multi_logloss: 1.97552\tvalid_1's multi_logloss: 2.07777\n",
      "[155]\ttraining's multi_logloss: 1.97223\tvalid_1's multi_logloss: 2.07722\n",
      "[160]\ttraining's multi_logloss: 1.96905\tvalid_1's multi_logloss: 2.07672\n",
      "[165]\ttraining's multi_logloss: 1.96571\tvalid_1's multi_logloss: 2.07602\n",
      "[170]\ttraining's multi_logloss: 1.96261\tvalid_1's multi_logloss: 2.0756\n",
      "[175]\ttraining's multi_logloss: 1.95959\tvalid_1's multi_logloss: 2.07508\n",
      "[180]\ttraining's multi_logloss: 1.95658\tvalid_1's multi_logloss: 2.0746\n",
      "[185]\ttraining's multi_logloss: 1.95375\tvalid_1's multi_logloss: 2.07427\n",
      "[190]\ttraining's multi_logloss: 1.9508\tvalid_1's multi_logloss: 2.07386\n",
      "[195]\ttraining's multi_logloss: 1.94806\tvalid_1's multi_logloss: 2.07356\n",
      "[200]\ttraining's multi_logloss: 1.94526\tvalid_1's multi_logloss: 2.0732\n",
      "[205]\ttraining's multi_logloss: 1.94263\tvalid_1's multi_logloss: 2.07294\n",
      "[210]\ttraining's multi_logloss: 1.93986\tvalid_1's multi_logloss: 2.07255\n",
      "[215]\ttraining's multi_logloss: 1.93734\tvalid_1's multi_logloss: 2.0723\n",
      "[220]\ttraining's multi_logloss: 1.93476\tvalid_1's multi_logloss: 2.07206\n",
      "[225]\ttraining's multi_logloss: 1.93218\tvalid_1's multi_logloss: 2.07177\n",
      "[230]\ttraining's multi_logloss: 1.9296\tvalid_1's multi_logloss: 2.07146\n",
      "[235]\ttraining's multi_logloss: 1.9271\tvalid_1's multi_logloss: 2.07126\n",
      "[240]\ttraining's multi_logloss: 1.92473\tvalid_1's multi_logloss: 2.07118\n",
      "[245]\ttraining's multi_logloss: 1.92236\tvalid_1's multi_logloss: 2.07104\n",
      "[250]\ttraining's multi_logloss: 1.92002\tvalid_1's multi_logloss: 2.07089\n",
      "[255]\ttraining's multi_logloss: 1.91769\tvalid_1's multi_logloss: 2.07074\n",
      "[260]\ttraining's multi_logloss: 1.91532\tvalid_1's multi_logloss: 2.07059\n",
      "[265]\ttraining's multi_logloss: 1.91305\tvalid_1's multi_logloss: 2.07046\n",
      "[270]\ttraining's multi_logloss: 1.91076\tvalid_1's multi_logloss: 2.07031\n",
      "[275]\ttraining's multi_logloss: 1.90842\tvalid_1's multi_logloss: 2.07013\n",
      "[280]\ttraining's multi_logloss: 1.90608\tvalid_1's multi_logloss: 2.06997\n",
      "[285]\ttraining's multi_logloss: 1.9039\tvalid_1's multi_logloss: 2.06987\n",
      "[290]\ttraining's multi_logloss: 1.90174\tvalid_1's multi_logloss: 2.06985\n",
      "[295]\ttraining's multi_logloss: 1.89953\tvalid_1's multi_logloss: 2.06968\n",
      "[300]\ttraining's multi_logloss: 1.89739\tvalid_1's multi_logloss: 2.06956\n",
      "[305]\ttraining's multi_logloss: 1.89519\tvalid_1's multi_logloss: 2.06941\n",
      "[310]\ttraining's multi_logloss: 1.89311\tvalid_1's multi_logloss: 2.0693\n",
      "[315]\ttraining's multi_logloss: 1.89104\tvalid_1's multi_logloss: 2.0692\n",
      "[320]\ttraining's multi_logloss: 1.88897\tvalid_1's multi_logloss: 2.06913\n",
      "[325]\ttraining's multi_logloss: 1.88695\tvalid_1's multi_logloss: 2.06908\n",
      "[330]\ttraining's multi_logloss: 1.88494\tvalid_1's multi_logloss: 2.069\n",
      "[335]\ttraining's multi_logloss: 1.88292\tvalid_1's multi_logloss: 2.06893\n",
      "[340]\ttraining's multi_logloss: 1.88089\tvalid_1's multi_logloss: 2.06889\n",
      "[345]\ttraining's multi_logloss: 1.87896\tvalid_1's multi_logloss: 2.06885\n",
      "[350]\ttraining's multi_logloss: 1.87698\tvalid_1's multi_logloss: 2.0688\n",
      "[355]\ttraining's multi_logloss: 1.87496\tvalid_1's multi_logloss: 2.06869\n",
      "[360]\ttraining's multi_logloss: 1.87304\tvalid_1's multi_logloss: 2.06865\n",
      "[365]\ttraining's multi_logloss: 1.87105\tvalid_1's multi_logloss: 2.06858\n",
      "[370]\ttraining's multi_logloss: 1.86913\tvalid_1's multi_logloss: 2.06855\n",
      "[375]\ttraining's multi_logloss: 1.86723\tvalid_1's multi_logloss: 2.06849\n",
      "[380]\ttraining's multi_logloss: 1.8653\tvalid_1's multi_logloss: 2.06845\n",
      "[385]\ttraining's multi_logloss: 1.86342\tvalid_1's multi_logloss: 2.06839\n",
      "[390]\ttraining's multi_logloss: 1.86149\tvalid_1's multi_logloss: 2.06838\n",
      "[395]\ttraining's multi_logloss: 1.85958\tvalid_1's multi_logloss: 2.06834\n",
      "[400]\ttraining's multi_logloss: 1.85774\tvalid_1's multi_logloss: 2.0683\n",
      "[405]\ttraining's multi_logloss: 1.85587\tvalid_1's multi_logloss: 2.06822\n",
      "[410]\ttraining's multi_logloss: 1.85406\tvalid_1's multi_logloss: 2.06821\n",
      "[415]\ttraining's multi_logloss: 1.85225\tvalid_1's multi_logloss: 2.06822\n",
      "[420]\ttraining's multi_logloss: 1.85044\tvalid_1's multi_logloss: 2.06819\n",
      "[425]\ttraining's multi_logloss: 1.84865\tvalid_1's multi_logloss: 2.06816\n",
      "[430]\ttraining's multi_logloss: 1.84686\tvalid_1's multi_logloss: 2.06814\n",
      "[435]\ttraining's multi_logloss: 1.84505\tvalid_1's multi_logloss: 2.06813\n",
      "[440]\ttraining's multi_logloss: 1.84324\tvalid_1's multi_logloss: 2.06814\n",
      "[445]\ttraining's multi_logloss: 1.84145\tvalid_1's multi_logloss: 2.0681\n",
      "[450]\ttraining's multi_logloss: 1.83967\tvalid_1's multi_logloss: 2.06809\n",
      "[455]\ttraining's multi_logloss: 1.83795\tvalid_1's multi_logloss: 2.06811\n",
      "[460]\ttraining's multi_logloss: 1.83623\tvalid_1's multi_logloss: 2.06812\n",
      "[465]\ttraining's multi_logloss: 1.83447\tvalid_1's multi_logloss: 2.06814\n",
      "[470]\ttraining's multi_logloss: 1.83275\tvalid_1's multi_logloss: 2.06812\n",
      "[475]\ttraining's multi_logloss: 1.83108\tvalid_1's multi_logloss: 2.06813\n",
      "[480]\ttraining's multi_logloss: 1.82938\tvalid_1's multi_logloss: 2.06813\n",
      "[485]\ttraining's multi_logloss: 1.82766\tvalid_1's multi_logloss: 2.0681\n",
      "[490]\ttraining's multi_logloss: 1.82602\tvalid_1's multi_logloss: 2.06814\n",
      "[495]\ttraining's multi_logloss: 1.82435\tvalid_1's multi_logloss: 2.06813\n",
      "[500]\ttraining's multi_logloss: 1.82273\tvalid_1's multi_logloss: 2.06816\n",
      "[505]\ttraining's multi_logloss: 1.82104\tvalid_1's multi_logloss: 2.06816\n",
      "[510]\ttraining's multi_logloss: 1.81937\tvalid_1's multi_logloss: 2.06815\n",
      "[515]\ttraining's multi_logloss: 1.81769\tvalid_1's multi_logloss: 2.06814\n",
      "[520]\ttraining's multi_logloss: 1.81605\tvalid_1's multi_logloss: 2.06815\n",
      "[525]\ttraining's multi_logloss: 1.81443\tvalid_1's multi_logloss: 2.06817\n",
      "[530]\ttraining's multi_logloss: 1.81276\tvalid_1's multi_logloss: 2.06817\n",
      "[535]\ttraining's multi_logloss: 1.8111\tvalid_1's multi_logloss: 2.06816\n",
      "[540]\ttraining's multi_logloss: 1.8095\tvalid_1's multi_logloss: 2.0682\n",
      "[545]\ttraining's multi_logloss: 1.80788\tvalid_1's multi_logloss: 2.0682\n",
      "Early stopping, best iteration is:\n",
      "[449]\ttraining's multi_logloss: 1.84003\tvalid_1's multi_logloss: 2.06808\n",
      "val rmse: 465.4730563242798\n",
      "val mae: 288.48648369849013\n",
      "runtime: 4087.4645869731903\n",
      "\n",
      "1 fold...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[5]\ttraining's multi_logloss: 2.39753\tvalid_1's multi_logloss: 2.40556\n",
      "[10]\ttraining's multi_logloss: 2.32084\tvalid_1's multi_logloss: 2.33405\n",
      "[15]\ttraining's multi_logloss: 2.26482\tvalid_1's multi_logloss: 2.28226\n",
      "[20]\ttraining's multi_logloss: 2.22321\tvalid_1's multi_logloss: 2.24461\n",
      "[25]\ttraining's multi_logloss: 2.18973\tvalid_1's multi_logloss: 2.21488\n",
      "[30]\ttraining's multi_logloss: 2.16329\tvalid_1's multi_logloss: 2.1921\n",
      "[35]\ttraining's multi_logloss: 2.1409\tvalid_1's multi_logloss: 2.17324\n",
      "[40]\ttraining's multi_logloss: 2.12288\tvalid_1's multi_logloss: 2.15871\n",
      "[45]\ttraining's multi_logloss: 2.10758\tvalid_1's multi_logloss: 2.14677\n",
      "[50]\ttraining's multi_logloss: 2.0941\tvalid_1's multi_logloss: 2.13668\n",
      "[55]\ttraining's multi_logloss: 2.08313\tvalid_1's multi_logloss: 2.12911\n",
      "[60]\ttraining's multi_logloss: 2.07302\tvalid_1's multi_logloss: 2.1223\n",
      "[65]\ttraining's multi_logloss: 2.06312\tvalid_1's multi_logloss: 2.11571\n",
      "[70]\ttraining's multi_logloss: 2.05532\tvalid_1's multi_logloss: 2.11119\n",
      "[75]\ttraining's multi_logloss: 2.0476\tvalid_1's multi_logloss: 2.10673\n",
      "[80]\ttraining's multi_logloss: 2.04108\tvalid_1's multi_logloss: 2.10339\n",
      "[85]\ttraining's multi_logloss: 2.03429\tvalid_1's multi_logloss: 2.09979\n",
      "[90]\ttraining's multi_logloss: 2.02797\tvalid_1's multi_logloss: 2.09675\n",
      "[95]\ttraining's multi_logloss: 2.02205\tvalid_1's multi_logloss: 2.09409\n",
      "[100]\ttraining's multi_logloss: 2.01673\tvalid_1's multi_logloss: 2.09186\n",
      "[105]\ttraining's multi_logloss: 2.01167\tvalid_1's multi_logloss: 2.08995\n",
      "[110]\ttraining's multi_logloss: 2.00672\tvalid_1's multi_logloss: 2.0881\n",
      "[115]\ttraining's multi_logloss: 2.00206\tvalid_1's multi_logloss: 2.08646\n",
      "[120]\ttraining's multi_logloss: 1.99796\tvalid_1's multi_logloss: 2.08531\n",
      "[125]\ttraining's multi_logloss: 1.99361\tvalid_1's multi_logloss: 2.08398\n",
      "[130]\ttraining's multi_logloss: 1.9898\tvalid_1's multi_logloss: 2.08308\n",
      "[135]\ttraining's multi_logloss: 1.98596\tvalid_1's multi_logloss: 2.08213\n",
      "[140]\ttraining's multi_logloss: 1.98223\tvalid_1's multi_logloss: 2.08119\n",
      "[145]\ttraining's multi_logloss: 1.97867\tvalid_1's multi_logloss: 2.08039\n",
      "[150]\ttraining's multi_logloss: 1.97524\tvalid_1's multi_logloss: 2.07967\n",
      "[155]\ttraining's multi_logloss: 1.97192\tvalid_1's multi_logloss: 2.07902\n",
      "[160]\ttraining's multi_logloss: 1.9686\tvalid_1's multi_logloss: 2.07837\n",
      "[165]\ttraining's multi_logloss: 1.96538\tvalid_1's multi_logloss: 2.0777\n",
      "[170]\ttraining's multi_logloss: 1.96238\tvalid_1's multi_logloss: 2.0773\n",
      "[175]\ttraining's multi_logloss: 1.95925\tvalid_1's multi_logloss: 2.0767\n",
      "[180]\ttraining's multi_logloss: 1.95634\tvalid_1's multi_logloss: 2.07633\n",
      "[185]\ttraining's multi_logloss: 1.95339\tvalid_1's multi_logloss: 2.0759\n",
      "[190]\ttraining's multi_logloss: 1.95057\tvalid_1's multi_logloss: 2.07559\n",
      "[195]\ttraining's multi_logloss: 1.94784\tvalid_1's multi_logloss: 2.07531\n",
      "[200]\ttraining's multi_logloss: 1.94505\tvalid_1's multi_logloss: 2.07496\n",
      "[205]\ttraining's multi_logloss: 1.94245\tvalid_1's multi_logloss: 2.07475\n",
      "[210]\ttraining's multi_logloss: 1.93978\tvalid_1's multi_logloss: 2.0744\n",
      "[215]\ttraining's multi_logloss: 1.93726\tvalid_1's multi_logloss: 2.07421\n",
      "[220]\ttraining's multi_logloss: 1.93473\tvalid_1's multi_logloss: 2.07405\n",
      "[225]\ttraining's multi_logloss: 1.93218\tvalid_1's multi_logloss: 2.07379\n",
      "[230]\ttraining's multi_logloss: 1.92976\tvalid_1's multi_logloss: 2.07365\n",
      "[235]\ttraining's multi_logloss: 1.92727\tvalid_1's multi_logloss: 2.07339\n",
      "[240]\ttraining's multi_logloss: 1.92485\tvalid_1's multi_logloss: 2.07323\n",
      "[245]\ttraining's multi_logloss: 1.92245\tvalid_1's multi_logloss: 2.07304\n",
      "[250]\ttraining's multi_logloss: 1.92005\tvalid_1's multi_logloss: 2.07292\n",
      "[255]\ttraining's multi_logloss: 1.9177\tvalid_1's multi_logloss: 2.07276\n",
      "[260]\ttraining's multi_logloss: 1.91532\tvalid_1's multi_logloss: 2.07258\n",
      "[265]\ttraining's multi_logloss: 1.91296\tvalid_1's multi_logloss: 2.07242\n",
      "[270]\ttraining's multi_logloss: 1.91057\tvalid_1's multi_logloss: 2.07219\n",
      "[275]\ttraining's multi_logloss: 1.90837\tvalid_1's multi_logloss: 2.07206\n",
      "[280]\ttraining's multi_logloss: 1.9061\tvalid_1's multi_logloss: 2.07193\n",
      "[285]\ttraining's multi_logloss: 1.90384\tvalid_1's multi_logloss: 2.07182\n",
      "[290]\ttraining's multi_logloss: 1.90158\tvalid_1's multi_logloss: 2.07166\n",
      "[295]\ttraining's multi_logloss: 1.89944\tvalid_1's multi_logloss: 2.0716\n",
      "[300]\ttraining's multi_logloss: 1.89735\tvalid_1's multi_logloss: 2.07156\n",
      "[305]\ttraining's multi_logloss: 1.89523\tvalid_1's multi_logloss: 2.07151\n",
      "[310]\ttraining's multi_logloss: 1.8931\tvalid_1's multi_logloss: 2.07141\n",
      "[315]\ttraining's multi_logloss: 1.891\tvalid_1's multi_logloss: 2.07136\n",
      "[320]\ttraining's multi_logloss: 1.88892\tvalid_1's multi_logloss: 2.07127\n",
      "[325]\ttraining's multi_logloss: 1.88688\tvalid_1's multi_logloss: 2.07124\n",
      "[330]\ttraining's multi_logloss: 1.88484\tvalid_1's multi_logloss: 2.07116\n",
      "[335]\ttraining's multi_logloss: 1.88285\tvalid_1's multi_logloss: 2.07112\n",
      "[340]\ttraining's multi_logloss: 1.88083\tvalid_1's multi_logloss: 2.07109\n",
      "[345]\ttraining's multi_logloss: 1.87882\tvalid_1's multi_logloss: 2.07104\n",
      "[350]\ttraining's multi_logloss: 1.8769\tvalid_1's multi_logloss: 2.07102\n",
      "[355]\ttraining's multi_logloss: 1.87494\tvalid_1's multi_logloss: 2.07094\n",
      "[360]\ttraining's multi_logloss: 1.873\tvalid_1's multi_logloss: 2.07092\n",
      "[365]\ttraining's multi_logloss: 1.87097\tvalid_1's multi_logloss: 2.07084\n",
      "[370]\ttraining's multi_logloss: 1.86902\tvalid_1's multi_logloss: 2.0708\n",
      "[375]\ttraining's multi_logloss: 1.86706\tvalid_1's multi_logloss: 2.07075\n",
      "[380]\ttraining's multi_logloss: 1.86515\tvalid_1's multi_logloss: 2.07071\n",
      "[385]\ttraining's multi_logloss: 1.86321\tvalid_1's multi_logloss: 2.07065\n",
      "[390]\ttraining's multi_logloss: 1.86136\tvalid_1's multi_logloss: 2.07064\n",
      "[395]\ttraining's multi_logloss: 1.85949\tvalid_1's multi_logloss: 2.07062\n",
      "[400]\ttraining's multi_logloss: 1.85765\tvalid_1's multi_logloss: 2.0706\n",
      "[405]\ttraining's multi_logloss: 1.85573\tvalid_1's multi_logloss: 2.07059\n",
      "[410]\ttraining's multi_logloss: 1.85387\tvalid_1's multi_logloss: 2.07055\n",
      "[415]\ttraining's multi_logloss: 1.85208\tvalid_1's multi_logloss: 2.07053\n",
      "[420]\ttraining's multi_logloss: 1.85029\tvalid_1's multi_logloss: 2.07055\n",
      "[425]\ttraining's multi_logloss: 1.84847\tvalid_1's multi_logloss: 2.07052\n",
      "[430]\ttraining's multi_logloss: 1.84664\tvalid_1's multi_logloss: 2.07047\n",
      "[435]\ttraining's multi_logloss: 1.84482\tvalid_1's multi_logloss: 2.07041\n",
      "[440]\ttraining's multi_logloss: 1.84309\tvalid_1's multi_logloss: 2.07041\n",
      "[445]\ttraining's multi_logloss: 1.84131\tvalid_1's multi_logloss: 2.07038\n",
      "[450]\ttraining's multi_logloss: 1.83952\tvalid_1's multi_logloss: 2.07035\n",
      "[455]\ttraining's multi_logloss: 1.83781\tvalid_1's multi_logloss: 2.07035\n",
      "[460]\ttraining's multi_logloss: 1.83604\tvalid_1's multi_logloss: 2.07036\n",
      "[465]\ttraining's multi_logloss: 1.8343\tvalid_1's multi_logloss: 2.07033\n",
      "[470]\ttraining's multi_logloss: 1.83257\tvalid_1's multi_logloss: 2.07035\n",
      "[475]\ttraining's multi_logloss: 1.83088\tvalid_1's multi_logloss: 2.07034\n",
      "[480]\ttraining's multi_logloss: 1.82913\tvalid_1's multi_logloss: 2.07028\n",
      "[485]\ttraining's multi_logloss: 1.82736\tvalid_1's multi_logloss: 2.07027\n",
      "[490]\ttraining's multi_logloss: 1.82565\tvalid_1's multi_logloss: 2.07026\n",
      "[495]\ttraining's multi_logloss: 1.82394\tvalid_1's multi_logloss: 2.07026\n",
      "[500]\ttraining's multi_logloss: 1.82227\tvalid_1's multi_logloss: 2.07025\n",
      "[505]\ttraining's multi_logloss: 1.82059\tvalid_1's multi_logloss: 2.07024\n",
      "[510]\ttraining's multi_logloss: 1.81892\tvalid_1's multi_logloss: 2.07021\n",
      "[515]\ttraining's multi_logloss: 1.81727\tvalid_1's multi_logloss: 2.07019\n",
      "[520]\ttraining's multi_logloss: 1.81558\tvalid_1's multi_logloss: 2.07016\n",
      "[525]\ttraining's multi_logloss: 1.81394\tvalid_1's multi_logloss: 2.07017\n",
      "[530]\ttraining's multi_logloss: 1.81234\tvalid_1's multi_logloss: 2.07018\n",
      "[535]\ttraining's multi_logloss: 1.81069\tvalid_1's multi_logloss: 2.07018\n",
      "[540]\ttraining's multi_logloss: 1.80907\tvalid_1's multi_logloss: 2.07021\n",
      "[545]\ttraining's multi_logloss: 1.80745\tvalid_1's multi_logloss: 2.07023\n",
      "[550]\ttraining's multi_logloss: 1.80584\tvalid_1's multi_logloss: 2.07022\n",
      "[555]\ttraining's multi_logloss: 1.80423\tvalid_1's multi_logloss: 2.07021\n",
      "[560]\ttraining's multi_logloss: 1.80262\tvalid_1's multi_logloss: 2.07022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[565]\ttraining's multi_logloss: 1.80105\tvalid_1's multi_logloss: 2.07023\n",
      "[570]\ttraining's multi_logloss: 1.79946\tvalid_1's multi_logloss: 2.07025\n",
      "[575]\ttraining's multi_logloss: 1.79783\tvalid_1's multi_logloss: 2.07027\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from scipy.stats import kurtosis\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "'''\n",
    "    方案思路：33分类。\n",
    "              label为还款日期距成交日期的天数，可能的情况有0天到31天，未还款定义为32，一共33个类别。\n",
    "              预测出每个label对应的概率，然后分别乘以应还的金额，就是每天需要还的金额。\n",
    "    线上分数：8500左右。\n",
    "              特征还有很多可以做，并且behavior表还没用，repay_logs表也还有很多有价值的东西没提取，因此分数还能提高。\n",
    "    主要问题：线下验证分数不靠谱，线上波动很大。\n",
    "              线下分类acc很低，需要找准特征工程的方向来提高分类准确率，此题当作分类任务和当作回归任务时的特征工程方向可能差别很大。\n",
    "    作者：天才儿童。\n",
    "'''\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('/Users/szkfzx/datasets/papadai/train.csv', parse_dates=['auditing_date', 'due_date', 'repay_date'])\n",
    "train_df['repay_date'] = train_df[['due_date', 'repay_date']].apply(\n",
    "    lambda x: x['repay_date'] if x['repay_date'] != '\\\\N' else x['due_date'], axis=1\n",
    ")\n",
    "train_df['repay_amt'] = train_df['repay_amt'].apply(lambda x: x if x != '\\\\N' else 0).astype('float32')\n",
    "train_df['label'] = (train_df['repay_date'] - train_df['auditing_date']).dt.days\n",
    "train_df.loc[train_df['repay_amt'] == 0, 'label'] = 32\n",
    "clf_labels = train_df['label'].values\n",
    "amt_labels = train_df['repay_amt'].values\n",
    "del train_df['label'], train_df['repay_amt'], train_df['repay_date']\n",
    "train_due_amt_df = train_df[['due_amt']]\n",
    "train_num = train_df.shape[0]\n",
    "test_df = pd.read_csv('/Users/szkfzx/datasets/papadai/test.csv', parse_dates=['auditing_date', 'due_date'])\n",
    "sub = test_df[['listing_id', 'auditing_date', 'due_amt']]\n",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "listing_info_df = pd.read_csv('/Users/szkfzx/datasets/papadai/listing_info.csv')\n",
    "del listing_info_df['user_id'], listing_info_df['auditing_date']\n",
    "df = df.merge(listing_info_df, on='listing_id', how='left')\n",
    "\n",
    "# 表中有少数user不止一条记录，因此按日期排序，去重，只保留最新的一条记录。\n",
    "user_info_df = pd.read_csv('/Users/szkfzx/datasets/papadai/user_info.csv', parse_dates=['reg_mon', 'insertdate'])\n",
    "user_info_df.rename(columns={'insertdate': 'info_insert_date'}, inplace=True)\n",
    "user_info_df = user_info_df.sort_values(by='info_insert_date', ascending=False).drop_duplicates('user_id').reset_index(drop=True)\n",
    "df = df.merge(user_info_df, on='user_id', how='left')\n",
    "\n",
    "# 同上\n",
    "user_tag_df = pd.read_csv('/Users/szkfzx/datasets/papadai/user_taglist.csv', parse_dates=['insertdate'])\n",
    "user_tag_df.rename(columns={'insertdate': 'tag_insert_date'}, inplace=True)\n",
    "user_tag_df = user_tag_df.sort_values(by='tag_insert_date', ascending=False).drop_duplicates('user_id').reset_index(drop=True)\n",
    "df = df.merge(user_tag_df, on='user_id', how='left')\n",
    "\n",
    "# 历史记录表能做的特征远不止这些\n",
    "repay_log_df = pd.read_csv('/Users/szkfzx/datasets/papadai/user_repay_logs.csv', parse_dates=['due_date', 'repay_date'])\n",
    "# 由于题目任务只预测第一期的还款情况，因此这里只保留第一期的历史记录。当然非第一期的记录也能提取很多特征。\n",
    "repay_log_df = repay_log_df[repay_log_df['order_id'] == 1].reset_index(drop=True)\n",
    "repay_log_df['repay'] = repay_log_df['repay_date'].astype('str').apply(lambda x: 1 if x != '2200-01-01' else 0)\n",
    "repay_log_df['early_repay_days'] = (repay_log_df['due_date'] - repay_log_df['repay_date']).dt.days\n",
    "repay_log_df['early_repay_days'] = repay_log_df['early_repay_days'].apply(lambda x: x if x >= 0 else -1)\n",
    "for f in ['listing_id', 'order_id', 'due_date', 'repay_date', 'repay_amt']:\n",
    "    del repay_log_df[f]\n",
    "group = repay_log_df.groupby('user_id', as_index=False)\n",
    "repay_log_df = repay_log_df.merge(\n",
    "    group['repay'].agg({'repay_mean': 'mean'}), on='user_id', how='left'\n",
    ")\n",
    "repay_log_df = repay_log_df.merge(\n",
    "    group['early_repay_days'].agg({\n",
    "        'early_repay_days_max': 'max', 'early_repay_days_median': 'median', 'early_repay_days_sum': 'sum',\n",
    "        'early_repay_days_mean': 'mean', 'early_repay_days_std': 'std'\n",
    "    }), on='user_id', how='left'\n",
    ")\n",
    "repay_log_df = repay_log_df.merge(\n",
    "    group['due_amt'].agg({\n",
    "        'due_amt_max': 'max', 'due_amt_min': 'min', 'due_amt_median': 'median',\n",
    "        'due_amt_mean': 'mean', 'due_amt_sum': 'sum', 'due_amt_std': 'std',\n",
    "        'due_amt_skew': 'skew', 'due_amt_kurt': kurtosis, 'due_amt_ptp': np.ptp\n",
    "    }), on='user_id', how='left'\n",
    ")\n",
    "del repay_log_df['repay'], repay_log_df['early_repay_days'], repay_log_df['due_amt']\n",
    "repay_log_df = repay_log_df.drop_duplicates('user_id').reset_index(drop=True)\n",
    "df = df.merge(repay_log_df, on='user_id', how='left')\n",
    "\n",
    "cate_cols = ['gender', 'cell_province', 'id_province', 'id_city']\n",
    "for f in cate_cols:\n",
    "    df[f] = df[f].map(dict(zip(df[f].unique(), range(df[f].nunique())))).astype('int32')\n",
    "\n",
    "df['due_amt_per_days'] = df['due_amt'] / (train_df['due_date'] - train_df['auditing_date']).dt.days\n",
    "date_cols = ['auditing_date', 'due_date', 'reg_mon', 'info_insert_date', 'tag_insert_date']\n",
    "for f in date_cols:\n",
    "    if f in ['reg_mon', 'info_insert_date', 'tag_insert_date']:\n",
    "        df[f + '_year'] = df[f].dt.year\n",
    "    df[f + '_month'] = df[f].dt.month\n",
    "    if f in ['auditing_date', 'due_date', 'info_insert_date', 'tag_insert_date']:\n",
    "        df[f + '_day'] = df[f].dt.day\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek\n",
    "df.drop(columns=date_cols, axis=1, inplace=True)\n",
    "\n",
    "df['taglist'] = df['taglist'].astype('str').apply(lambda x: x.strip().replace('|', ' ').strip())\n",
    "tag_cv = CountVectorizer(min_df=10, max_df=0.9).fit_transform(df['taglist'])\n",
    "\n",
    "del df['user_id'], df['listing_id'], df['taglist']\n",
    "\n",
    "df = pd.get_dummies(df, columns=cate_cols)\n",
    "df = sparse.hstack((df.values, tag_cv), format='csr', dtype='float32')\n",
    "train_values, test_values = df[:train_num], df[train_num:]\n",
    "\n",
    "\n",
    "print(train_values.shape)\n",
    "# 五折验证也可以改成一次验证，按时间划分训练集和验证集，以避免由于时序引起的数据穿越问题。\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "clf = LGBMClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=10000,\n",
    "    subsample=0.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=2019\n",
    ")\n",
    "amt_oof = np.zeros(train_num)\n",
    "prob_oof = np.zeros((train_num, 33))\n",
    "test_pred_prob = np.zeros((test_values.shape[0], 33))\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(train_values, clf_labels)):\n",
    "    print(i, 'fold...')\n",
    "    t = time.time()\n",
    "\n",
    "    trn_x, trn_y = train_values[trn_idx], clf_labels[trn_idx]\n",
    "    val_x, val_y = train_values[val_idx], clf_labels[val_idx]\n",
    "    val_repay_amt = amt_labels[val_idx]\n",
    "    val_due_amt = train_due_amt_df.iloc[val_idx]\n",
    "\n",
    "    clf.fit(\n",
    "        trn_x, trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        early_stopping_rounds=100, verbose=5\n",
    "    )\n",
    "    # shepe = (-1, 33)\n",
    "    val_pred_prob_everyday = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "    prob_oof[val_idx] = val_pred_prob_everyday\n",
    "    val_pred_prob_today = [val_pred_prob_everyday[i][val_y[i]] for i in range(val_pred_prob_everyday.shape[0])]\n",
    "    val_pred_repay_amt = val_due_amt['due_amt'].values * val_pred_prob_today\n",
    "    print('val rmse:', np.sqrt(mean_squared_error(val_repay_amt, val_pred_repay_amt)))\n",
    "    print('val mae:', mean_absolute_error(val_repay_amt, val_pred_repay_amt))\n",
    "    amt_oof[val_idx] = val_pred_repay_amt\n",
    "    test_pred_prob += clf.predict_proba(test_values, num_iteration=clf.best_iteration_) / skf.n_splits\n",
    "\n",
    "    print('runtime: {}\\n'.format(time.time() - t))\n",
    "\n",
    "print('\\ncv rmse:', np.sqrt(mean_squared_error(amt_labels, amt_oof)))\n",
    "print('cv mae:', mean_absolute_error(amt_labels, amt_oof))\n",
    "print('cv logloss:', log_loss(clf_labels, prob_oof))\n",
    "print('cv acc:', accuracy_score(clf_labels, np.argmax(prob_oof, axis=1)))\n",
    "\n",
    "prob_cols = ['prob_{}'.format(i) for i in range(33)]\n",
    "for i, f in enumerate(prob_cols):\n",
    "    sub[f] = test_pred_prob[:, i]\n",
    "sub_example = pd.read_csv('/Users/szkfzx/datasets/papadai/submission.csv', parse_dates=['repay_date'])\n",
    "sub_example = sub_example.merge(sub, on='listing_id', how='left')\n",
    "sub_example['days'] = (sub_example['repay_date'] - sub_example['auditing_date']).dt.days\n",
    "# shape = (-1, 33)\n",
    "test_prob = sub_example[prob_cols].values\n",
    "test_labels = sub_example['days'].values\n",
    "test_prob = [test_prob[i][test_labels[i]] for i in range(test_prob.shape[0])]\n",
    "sub_example['repay_amt'] = sub_example['due_amt'] * test_prob\n",
    "sub_example[['listing_id', 'repay_date', 'repay_amt']].to_csv('sub.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szkfzx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:17: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L(X, y, W): \n",
    "    \"\"\"  \n",
    "    fully‐vectorized implementation :  \n",
    "    ‐ X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR‐10)  \n",
    "    ‐ y is array of integers specifying correct class (e.g. 50,000‐D array)  \n",
    "    ‐ W are weights (e.g. 10 x 3073) \n",
    "    \"\"\"  \n",
    "    # evaluate loss over all examples in X without using any for loops  \n",
    "    # left as exercise to reader in the assignment\n",
    "    delta = 1.0  \n",
    "    scores = W.dot(X)\n",
    "    D = W.shape[0]\n",
    "    loss_i = loss_i = 0.0\n",
    "    for i in range(D):\n",
    "        loss_i = np.mean(np.sqrt(scores[i] - y))\n",
    "    return loss_i\n",
    "\n",
    "# 假设X_train的每一列都是一个数据样本（比如3073 x 50000）\n",
    "X_train = np.random.randn(3073,50000).astype('int64')\n",
    "# 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）\n",
    "Y_train = np.random.randn(1,50000).astype('int64')\n",
    "# 假设函数L对损失函数进行评价   \n",
    "bestloss = float(\"inf\") # Python assigns the highest possible float value \n",
    "for num in range(1000):  \n",
    "    W = np.random.randn(10,3073) * 0.0001 # generate random parameters  \n",
    "    loss = L(X_train,Y_train,W) # get the loss over the entire training set  \n",
    "    if loss < bestloss: # keep track of the best solution  \n",
    "        bestloss = loss  \n",
    "        bestW = W  \n",
    "        print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))   \n",
    "        # 输出: \n",
    "        # in attempt 0 the loss was 9.401632, best 9.401632 \n",
    "        # in attempt 1 the loss was 8.959668, best 8.959668 \n",
    "        # in attempt 2 the loss was 9.044034, best 8.959668 \n",
    "        # in attempt 3 the loss was 9.278948, best 8.959668 \n",
    "        # in attempt 4 the loss was 8.857370, best 8.857370 \n",
    "        # in attempt 5 the loss was 8.943151, best 8.857370 \n",
    "        # in attempt 6 the loss was 8.605604, best 8.605604 \n",
    "        # ... (trunctated: continues for 1000 lines)\n",
    "        \n",
    "# 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1] \n",
    "scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples \n",
    "# 找到在每列中评分值最大的索引（即预测的分类） \n",
    "Yte_predict = np.argmax(scores,axis = 0) \n",
    "# 以及计算准确率 \n",
    "np.mean(Yte_predict == Yte) \n",
    "# 返回 0.155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机本地搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-53b12351a760>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mWtry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3073\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYtr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mWtry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbestloss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWtry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.random.randn(10,3073) * 0.001 # 生成随机初始W \n",
    "bestloss = float(\"inf\") \n",
    "for i in range(1000):  \n",
    "    step_size = 0.0001  \n",
    "    Wtry = W + np.random.randn(10,3073) * step_size  \n",
    "    loss = L(Xtr_cols,Ytr,Wtry)  \n",
    "    if loss < bestloss:  \n",
    "        W = Wtry  \n",
    "        bestloss = loss  \n",
    "    print('iter %d loss is %f' % (i,bestloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 跟随梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x):  \n",
    "    \"\"\"   \n",
    "    一个f在x处的数值梯度法的简单实现  \n",
    "    ‐ f是只有一个参数的函数  \n",
    "    ‐ x是计算梯度的点  \n",
    "    \"\"\"     \n",
    "    fx = f(x) # 在原点计算函数值  \n",
    "    grad = np.zeros(x.shape)  \n",
    "    h = 0.00001    \n",
    "    \n",
    "    # 对x中所有的索引进行迭代  \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])  \n",
    "    while not it.finished:    \n",
    "        \n",
    "        # 计算x+h处的函数值  \n",
    "        ix = it.multi_index  \n",
    "        old_value = x[ix]  \n",
    "        x[ix] = old_value + h # 增加h  \n",
    "        fxh = f(x) # 计算f(x + h)  x\n",
    "        [ix] = old_value # 存到前一个值中 (非常重要)    \n",
    "        \n",
    "        # 计算偏导数  \n",
    "        grad[ix] = (fxh - fx) / h # 坡度  \n",
    "        it.iternext() # 到下个维度 \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2609f2d7f7ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3073\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m \u001b[1;31m# 随机权重向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 得到梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-9e6401859d4d>\u001b[0m in \u001b[0;36meval_numerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0;31m‐\u001b[0m \u001b[0mx是计算梯度的点\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"     \n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 在原点计算函数值\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2609f2d7f7ba>\u001b[0m in \u001b[0;36mCIFAR10_loss_fun\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# (在这里参数就是权重)所以也包含了X_train和Y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3073\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m \u001b[1;31m# 随机权重向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 得到梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "# 要使用上面的代码我们需要一个只有一个参数的函数 \n",
    "# (在这里参数就是权重)所以也包含了X_train和Y_train \n",
    "def CIFAR10_loss_fun(W):  \n",
    "    return L(X_train, Y_train, W)   \n",
    "W = np.random.rand(10, 3073) * 0.001 # 随机权重向量 \n",
    "df = eval_numerical_gradient(CIFAR10_loss_fun, W) # 得到梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4b1bf9d08976>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 初始损失值\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'original loss: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 查看不同步长的效果\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep_size_log\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstep_size_log\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mW_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;31m# 权重空间中的新位置\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2609f2d7f7ba>\u001b[0m in \u001b[0;36mCIFAR10_loss_fun\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# (在这里参数就是权重)所以也包含了X_train和Y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3073\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m \u001b[1;31m# 随机权重向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 得到梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "loss_original = CIFAR10_loss_fun(W) # 初始损失值 \n",
    "print('original loss: %f' % (loss_original, ))# 查看不同步长的效果 \n",
    "for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:  \n",
    "    step_size = 10 ** step_size_log  \n",
    "    W_new = W - step_size * df # 权重空间中的新位置  \n",
    "    loss_new = CIFAR10_loss_fun(W_new)  \n",
    "    print('for step size %f new loss: %f' % (step_size, loss_new))   \n",
    "    # 输出: \n",
    "    # original loss: 2.200718 \n",
    "    # for step size 1.000000e‐10 new loss: 2.200652 \n",
    "    # for step size 1.000000e‐09 new loss: 2.200057 \n",
    "    # for step size 1.000000e‐08 new loss: 2.194116 \n",
    "    # for step size 1.000000e‐07 new loss: 2.135493 \n",
    "    # for step size 1.000000e‐06 new loss: 1.647802\n",
    "    # for step size 1.000000e‐05 new loss: 2.844355\n",
    "    # for step size 1.000000e‐04 new loss: 25.558142 \n",
    "    # for step size 1.000000e‐03 new loss: 254.086573 \n",
    "    # for step size 1.000000e‐02 new loss: 2539.370888\n",
    "    # for step size 1.000000e‐01 new loss: 25392.214036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_gradient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ba8aa2995661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 普通的梯度下降\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mweights_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights_grad\u001b[0m \u001b[1;31m# 进行梯度更\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_gradient' is not defined"
     ]
    }
   ],
   "source": [
    "# 普通的梯度下降   \n",
    "while True:  \n",
    "    weights_grad = evaluate_gradient(loss_fun, data, weights)  \n",
    "    weights += - step_size * weights_grad # 进行梯度更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c2e2c7e4d173>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 普通的小批量数据梯度下降\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 256个数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mweights_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights_grad\u001b[0m \u001b[1;31m# 参数更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_training_data' is not defined"
     ]
    }
   ],
   "source": [
    "# 普通的小批量数据梯度下降   \n",
    "while True:  \n",
    "    data_batch = sample_training_data(data, 256) # 256个数据  \n",
    "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)  \n",
    "    weights += - step_size * weights_grad # 参数更新"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
